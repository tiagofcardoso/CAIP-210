// CAIP-210 Study Content - Lessons 7-9
// Clustering, Decision Trees, SVM

// Lesson 7: Building Clustering Models
STUDY_CONTENT[7] = {
    name: "Lesson 7: Building Clustering Models",
    icon: "üîµ",
    weight: "Focus Areas: k-Means, Hierarchical Clustering, Evaluation",
    topics: [
        {
            title: "k-Means Clustering",
            concept: `K-Means agrupa dados em k clusters baseado em dist√¢ncia:

üîÑ ALGORITMO:
1. Escolher k centr√≥ides iniciais (aleat√≥rio ou k-means++)
2. Atribuir cada ponto ao centr√≥ide mais pr√≥ximo
3. Recalcular centr√≥ides como m√©dia dos pontos
4. Repetir 2-3 at√© converg√™ncia

üìä ESCOLHENDO k:

ELBOW METHOD:
‚Ä¢ Plotar WCSS vs. k
‚Ä¢ WCSS = Within-Cluster Sum of Squares
‚Ä¢ Procurar "cotovelo" onde redu√ß√£o desacelera

SILHOUETTE SCORE:
‚Ä¢ Mede qu√£o similar ponto √© ao seu cluster vs. outros
‚Ä¢ Score: -1 a 1 (maior = melhor)
‚Ä¢ Usar k com maior silhouette m√©dio

‚öôÔ∏è k-MEANS++:
‚Ä¢ Inicializa√ß√£o mais inteligente
‚Ä¢ Primeiro centr√≥ide aleat√≥rio
‚Ä¢ Pr√≥ximos proporcionais √† dist√¢ncia
‚Ä¢ Evita converg√™ncia ruim

‚ö†Ô∏è LIMITA√á√ïES:
‚Ä¢ Assume clusters esf√©ricos
‚Ä¢ Sens√≠vel a escala
‚Ä¢ Sens√≠vel a outliers
‚Ä¢ N√∫mero k deve ser especificado`,
            keyPoints: [
                "K-means minimiza dist√¢ncia intra-cluster",
                "Elbow method: procurar ponto de inflex√£o",
                "Silhouette score: qualidade dos clusters",
                "k-means++: inicializa√ß√£o melhor que aleat√≥ria",
                "ESCALONAR features antes de clustering"
            ],
            example: `from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Escalonar dados (OBRIGAT√ìRIO!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Elbow Method
wcss = []
silhouettes = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))

# Plotar
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(K, wcss, 'bo-')
axes[0].set_xlabel('k')
axes[0].set_ylabel('WCSS')
axes[0].set_title('Elbow Method')

axes[1].plot(K, silhouettes, 'go-')
axes[1].set_xlabel('k')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('Silhouette Analysis')

# Melhor k (maior silhouette)
best_k = K[np.argmax(silhouettes)]
print(f"K √≥timo por silhouette: {best_k}")

# Modelo final
kmeans = KMeans(n_clusters=best_k, init='k-means++', random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# An√°lise dos clusters
for i in range(best_k):
    print(f"Cluster {i}: {np.sum(clusters == i)} pontos")
    print(f"  Centr√≥ide: {scaler.inverse_transform([kmeans.cluster_centers_[i]])[0]}")`,
            realCase: {
                title: "Segmenta√ß√£o de Clientes RFM",
                description: "Varejistas usam k-means em features RFM (Recency, Frequency, Monetary) para segmentar clientes em grupos como 'Champions', 'Loyal', 'At Risk', 'Lost'. Cada segmento recebe marketing diferenciado.",
                impact: "Aumenta ROI de marketing ao personalizar mensagens por segmento"
            }
        },
        {
            title: "Hierarchical Clustering",
            concept: `Cria hierarquia de clusters sem especificar k:

üìä DOIS TIPOS:

AGLOMERATIVO (bottom-up):
1. Cada ponto √© um cluster
2. Mesclar clusters mais pr√≥ximos
3. Repetir at√© um cluster
4. Dendrograma registra hierarquia

DIVISIVO (top-down):
1. Todos pontos em um cluster
2. Dividir cluster menos coeso
3. Repetir at√© clusters individuais

üîó LINKAGE (crit√©rio de proximidade):

SINGLE (nearest):
‚Ä¢ Dist√¢ncia m√≠nima entre pontos
‚Ä¢ Pode criar clusters alongados

COMPLETE (farthest):
‚Ä¢ Dist√¢ncia m√°xima entre pontos
‚Ä¢ Clusters mais compactos

AVERAGE:
‚Ä¢ M√©dia das dist√¢ncias
‚Ä¢ Equil√≠brio

WARD:
‚Ä¢ Minimiza vari√¢ncia intra-cluster
‚Ä¢ Clusters esf√©ricos, mais usado

üìà DENDROGRAMA:
‚Ä¢ Visualiza hierarquia
‚Ä¢ Cortar em altura h ‚Üí k clusters
‚Ä¢ Escolher h onde gap √© grande`,
            keyPoints: [
                "N√£o precisa especificar k antecipadamente",
                "Dendrograma mostra toda hierarquia",
                "Ward linkage: clusters compactos e esf√©ricos",
                "Cortar dendrograma na altura desejada",
                "Computacionalmente caro para grandes datasets"
            ],
            example: `from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import matplotlib.pyplot as plt

# Calcular linkage matrix
Z = linkage(X_scaled, method='ward')

# Plotar dendrograma
plt.figure(figsize=(12, 6))
dendrogram(Z, 
           truncate_mode='level', 
           p=5,
           leaf_rotation=90)
plt.title('Dendrograma Hier√°rquico')
plt.xlabel('Samples')
plt.ylabel('Dist√¢ncia')
plt.axhline(y=15, color='r', linestyle='--', label='Corte k=3')
plt.legend()

# Cortar em altura espec√≠fica
clusters_h = fcluster(Z, t=15, criterion='distance')

# Ou especificar n√∫mero de clusters
agg = AgglomerativeClustering(
    n_clusters=3,
    linkage='ward'
)
clusters = agg.fit_predict(X_scaled)

# Comparar diferentes linkages
for linkage_method in ['single', 'complete', 'average', 'ward']:
    agg = AgglomerativeClustering(n_clusters=3, linkage=linkage_method)
    labels = agg.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    print(f"{linkage_method}: Silhouette = {score:.3f}")`,
            realCase: {
                title: "Filogenia em Biologia",
                description: "Bi√≥logos usam clustering hier√°rquico para construir √°rvores filogen√©ticas mostrando rela√ß√µes evolutivas entre esp√©cies. O dendrograma representa a hist√≥ria evolutiva.",
                impact: "Fundamental para biologia evolutiva e taxonomia"
            }
        },
        {
            title: "Clustering Evaluation Metrics",
            concept: `M√©tricas para avaliar qualidade de clusters:

üìä M√âTRICAS INTERNAS (sem labels):

SILHOUETTE SCORE:
‚Ä¢ s = (b - a) / max(a, b)
‚Ä¢ a = dist√¢ncia m√©dia intra-cluster
‚Ä¢ b = dist√¢ncia m√©dia ao cluster mais pr√≥ximo
‚Ä¢ Range: -1 a 1 (maior = melhor)

DAVIES-BOULDIN INDEX:
‚Ä¢ Raz√£o dispers√£o intra / separa√ß√£o inter
‚Ä¢ Menor = melhor (clusters compactos e separados)

CALINSKI-HARABASZ (Variance Ratio):
‚Ä¢ BCSS / WCSS √ó (n - k) / (k - 1)
‚Ä¢ Maior = melhor

üìà M√âTRICAS EXTERNAS (com labels):

ADJUSTED RAND INDEX (ARI):
‚Ä¢ Similaridade com clustering "verdadeiro"
‚Ä¢ Range: -1 a 1 (1 = perfeito)

NORMALIZED MUTUAL INFORMATION (NMI):
‚Ä¢ Informa√ß√£o compartilhada vs. labels
‚Ä¢ Range: 0 a 1

‚ö†Ô∏è CONSIDERA√á√ïES:
‚Ä¢ M√©tricas internas preferem clusters esf√©ricos
‚Ä¢ Externas requerem labels (raramente dispon√≠veis)
‚Ä¢ Combinar m√∫ltiplas m√©tricas`,
            keyPoints: [
                "Silhouette: mais usado, interpret√°vel [-1, 1]",
                "Davies-Bouldin: menor √© melhor",
                "Calinski-Harabasz: maior √© melhor",
                "M√©tricas externas precisam de labels",
                "N√£o confiar em uma m√©trica apenas"
            ],
            example: `from sklearn.metrics import (silhouette_score, davies_bouldin_score,
                               calinski_harabasz_score,
                               adjusted_rand_score, 
                               normalized_mutual_info_score)

# Comparar diferentes k
results = []
for k in range(2, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    
    results.append({
        'k': k,
        'silhouette': silhouette_score(X_scaled, labels),
        'davies_bouldin': davies_bouldin_score(X_scaled, labels),
        'calinski_harabasz': calinski_harabasz_score(X_scaled, labels)
    })

import pandas as pd
df_results = pd.DataFrame(results)
print(df_results)

# Se tiver labels verdadeiros (raro em clustering real)
if y_true is not None:
    ari = adjusted_rand_score(y_true, labels)
    nmi = normalized_mutual_info_score(y_true, labels)
    print(f"ARI: {ari:.3f}")
    print(f"NMI: {nmi:.3f}")

# Silhouette por sample (identificar outliers)
from sklearn.metrics import silhouette_samples
sample_silhouettes = silhouette_samples(X_scaled, labels)
low_silhouette = np.where(sample_silhouettes < 0)[0]
print(f"Pontos mal clusterizados: {len(low_silhouette)}")`,
            realCase: {
                title: "Valida√ß√£o de Segmenta√ß√£o de Mercado",
                description: "Empresas de pesquisa de mercado usam m√∫ltiplas m√©tricas para validar segmenta√ß√µes. Al√©m de m√©tricas estat√≠sticas, consideram interpretabilidade de neg√≥cio e acionabilidade dos segmentos.",
                impact: "M√©tricas t√©cnicas + valida√ß√£o de neg√≥cio = segmenta√ß√£o √∫til"
            }
        }
    ]
};

// Lesson 8: Decision Trees and Random Forests
STUDY_CONTENT[8] = {
    name: "Lesson 8: Decision Trees & Random Forests",
    icon: "üå≤",
    weight: "Focus Areas: Tree Algorithms, Ensemble Learning, Feature Importance",
    topics: [
        {
            title: "Decision Trees",
            concept: `√Årvores de decis√£o dividem dados recursivamente:

üìä ESTRUTURA:
‚Ä¢ N√≥ raiz: todo o dataset
‚Ä¢ N√≥s internos: condi√ß√µes de divis√£o
‚Ä¢ Folhas: previs√µes finais
‚Ä¢ Ramos: resultados das condi√ß√µes

üîÄ CRIT√âRIOS DE DIVIS√ÉO:

GINI IMPURITY:
‚Ä¢ Gini = 1 - Œ£p·µ¢¬≤
‚Ä¢ 0 = puro (uma classe), 0.5 = m√°xima impureza
‚Ä¢ Usado por CART

ENTROPY / INFORMATION GAIN:
‚Ä¢ Entropy = -Œ£p·µ¢ log‚ÇÇ(p·µ¢)
‚Ä¢ IG = Entropy(pai) - Œ£(n‚±º/n)√óEntropy(filho‚±º)
‚Ä¢ Usado por ID3, C4.5

‚öôÔ∏è HIPERPAR√ÇMETROS:
‚Ä¢ max_depth: profundidade m√°xima
‚Ä¢ min_samples_split: m√≠nimo para dividir
‚Ä¢ min_samples_leaf: m√≠nimo nas folhas
‚Ä¢ max_features: features consideradas

üí° VANTAGENS:
‚Ä¢ Altamente interpret√°veis
‚Ä¢ N√£o requer escalonamento
‚Ä¢ Captura n√£o-linearidades
‚Ä¢ Feature importance built-in

‚ö†Ô∏è DESVANTAGENS:
‚Ä¢ Propensas a overfitting
‚Ä¢ Inst√°veis (pequenas mudan√ßas nos dados)
‚Ä¢ Fronteiras de decis√£o retil√≠neas`,
            keyPoints: [
                "Gini e Entropy: crit√©rios de divis√£o comuns",
                "max_depth controla complexidade (regulariza√ß√£o)",
                "N√£o precisa escalonar dados",
                "Muito interpret√°veis (exportar regras)",
                "Overfitting √© problema principal"
            ],
            example: `from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
import matplotlib.pyplot as plt

# Treinar √°rvore
tree = DecisionTreeClassifier(
    criterion='gini',
    max_depth=4,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)
tree.fit(X_train, y_train)

# Visualizar (se features nomeadas)
plt.figure(figsize=(20, 10))
plot_tree(tree, 
          feature_names=feature_names,
          class_names=['No', 'Yes'],
          filled=True,
          rounded=True)
plt.show()

# Extrair regras de decis√£o
rules = export_text(tree, feature_names=feature_names)
print(rules)

# Feature importance
importances = pd.DataFrame({
    'feature': feature_names,
    'importance': tree.feature_importances_
}).sort_values('importance', ascending=False)
print(importances)

# Comparar complexidades
for depth in [2, 4, 6, 8, None]:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    train_acc = tree.score(X_train, y_train)
    test_acc = tree.score(X_test, y_test)
    n_leaves = tree.get_n_leaves()
    print(f"Depth {depth}: Train={train_acc:.3f}, Test={test_acc:.3f}, Leaves={n_leaves}")`,
            realCase: {
                title: "Diagn√≥stico M√©dico Explic√°vel",
                description: "Hospitais usam √°rvores de decis√£o para triagem porque m√©dicos podem seguir e explicar as decis√µes. Regulamenta√ß√µes de sa√∫de frequentemente exigem modelos interpret√°veis.",
                impact: "Confian√ßa m√©dica + conformidade regulat√≥ria = ado√ß√£o cl√≠nica"
            }
        },
        {
            title: "Random Forests",
            concept: `Random Forest combina muitas √°rvores via bagging:

üå≤ ALGORITMO:
1. Criar N amostras bootstrap (com reposi√ß√£o)
2. Treinar √°rvore em cada amostra
3. Em cada n√≥, considerar apenas ‚àöfeatures aleat√≥rias
4. Agregar previs√µes (vota√ß√£o ou m√©dia)

üìä COMPONENTES:

BAGGING (Bootstrap Aggregating):
‚Ä¢ Amostrar com reposi√ß√£o
‚Ä¢ ~63% dos dados em cada √°rvore
‚Ä¢ Reduz vari√¢ncia

FEATURE RANDOMNESS:
‚Ä¢ Cada split considera subconjunto de features
‚Ä¢ Decorrelaciona as √°rvores
‚Ä¢ Torna ensemble mais robusto

üí° VANTAGENS:
‚Ä¢ Muito menos overfitting que √°rvore √∫nica
‚Ä¢ Robusto a outliers
‚Ä¢ Feature importance agregada
‚Ä¢ Out-of-bag (OOB) error como valida√ß√£o

‚öôÔ∏è HIPERPAR√ÇMETROS:
‚Ä¢ n_estimators: n√∫mero de √°rvores
‚Ä¢ max_features: features por split
‚Ä¢ max_depth: profundidade das √°rvores
‚Ä¢ min_samples_split/leaf: regulariza√ß√£o`,
            keyPoints: [
                "Ensemble de √°rvores via bootstrap sampling",
                "Feature randomness decorrelaciona √°rvores",
                "OOB error: valida√ß√£o gratuita sem split",
                "Mais √°rvores = melhor (at√© certo ponto)",
                "Menos interpret√°vel que √°rvore √∫nica"
            ],
            example: `from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Random Forest
rf = RandomForestClassifier(
    n_estimators=100,
    max_features='sqrt',  # ‚àön_features
    max_depth=10,
    min_samples_split=5,
    oob_score=True,  # usar OOB para valida√ß√£o
    random_state=42,
    n_jobs=-1  # paralelizar
)
rf.fit(X_train, y_train)

# OOB Score (valida√ß√£o gratuita!)
print(f"OOB Score: {rf.oob_score_:.3f}")
print(f"Test Score: {rf.score(X_test, y_test):.3f}")

# Feature Importance
importances = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importances['feature'][:15], importances['importance'][:15])
plt.xlabel('Importance')
plt.title('Top 15 Feature Importances')
plt.gca().invert_yaxis()

# Encontrar n_estimators √≥timo
errors = []
for n in range(10, 200, 10):
    rf = RandomForestClassifier(n_estimators=n, oob_score=True, random_state=42)
    rf.fit(X_train, y_train)
    errors.append(1 - rf.oob_score_)

plt.plot(range(10, 200, 10), errors)
plt.xlabel('n_estimators')
plt.ylabel('OOB Error')`,
            realCase: {
                title: "Detec√ß√£o de Fraude em Transa√ß√µes",
                description: "Bancos usam Random Forests para detectar fraude por sua robustez e capacidade de lidar com features mistas (categ√≥ricas + num√©ricas) sem pr√©-processamento extenso.",
                impact: "Alta precis√£o + feature importance = detec√ß√£o confi√°vel"
            }
        },
        {
            title: "Gradient Boosting",
            concept: `Boosting treina modelos sequencialmente para corrigir erros:

üìä ALGORITMO:
1. Treinar modelo inicial (previs√£o simples)
2. Calcular res√≠duos (erros)
3. Treinar pr√≥ximo modelo nos res√≠duos
4. Adicionar ao ensemble com learning rate
5. Repetir at√© N modelos

üí° DIFEREN√áA DE BAGGING:
‚Ä¢ Bagging: modelos paralelos, independentes
‚Ä¢ Boosting: modelos sequenciais, corrigem erros

üìà IMPLEMENTA√á√ïES:

GRADIENT BOOSTING (sklearn):
‚Ä¢ Implementa√ß√£o b√°sica
‚Ä¢ Relativamente lento

XGBOOST:
‚Ä¢ Regulariza√ß√£o L1/L2
‚Ä¢ Tratamento de missing values
‚Ä¢ Paralelizado, muito r√°pido

LIGHTGBM:
‚Ä¢ Crescimento leaf-wise
‚Ä¢ Ainda mais r√°pido
‚Ä¢ √ìtimo para grandes datasets

CATBOOST:
‚Ä¢ Excelente para categ√≥ricas
‚Ä¢ Menos overfitting

‚öôÔ∏è HIPERPAR√ÇMETROS CHAVE:
‚Ä¢ n_estimators: n√∫mero de √°rvores
‚Ä¢ learning_rate: contribui√ß√£o de cada √°rvore
‚Ä¢ max_depth: profundidade (menor que RF)`,
            keyPoints: [
                "Boosting: modelos sequenciais corrigindo erros",
                "Learning rate √ó n_estimators: trade-off",
                "XGBoost/LightGBM: estado da arte para tabulares",
                "max_depth geralmente menor que Random Forest",
                "Early stopping previne overfitting"
            ],
            example: `from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb

# Gradient Boosting (sklearn)
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gb.fit(X_train, y_train)

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    use_label_encoder=False,
    eval_metric='logloss'
)
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=10,
    verbose=False
)
print(f"Best iteration: {xgb_model.best_iteration}")

# LightGBM
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
lgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    callbacks=[lgb.early_stopping(10)]
)

# Comparar
models = {'GradientBoosting': gb, 'XGBoost': xgb_model, 'LightGBM': lgb_model}
for name, model in models.items():
    print(f"{name}: {model.score(X_test, y_test):.3f}")`,
            realCase: {
                title: "Kaggle Competitions",
                description: "XGBoost e LightGBM dominam competi√ß√µes Kaggle para dados tabulares. O XGBoost foi usado por 17 dos 29 vencedores em 2015, estabelecendo-se como padr√£o da ind√∫stria.",
                impact: "Gradient boosting √© estado da arte para dados tabulares"
            }
        }
    ]
};

// Lesson 9: Support Vector Machines
STUDY_CONTENT[9] = {
    name: "Lesson 9: Building Support Vector Machines",
    icon: "üìê",
    weight: "Focus Areas: SVM Classification, Kernel Trick, SVM Regression",
    topics: [
        {
            title: "SVM for Classification",
            concept: `SVM encontra hiperplano que maximiza margem entre classes:

üìä CONCEITOS FUNDAMENTAIS:

HIPERPLANO:
‚Ä¢ Fronteira de decis√£o que separa classes
‚Ä¢ Em 2D: linha, em 3D: plano, em nD: hiperplano

MARGEM:
‚Ä¢ Dist√¢ncia entre hiperplano e pontos mais pr√≥ximos
‚Ä¢ SVM maximiza esta margem
‚Ä¢ Maior margem = melhor generaliza√ß√£o

VETORES DE SUPORTE:
‚Ä¢ Pontos mais pr√≥ximos do hiperplano
‚Ä¢ Definem a margem
‚Ä¢ √önicos pontos que importam para decis√£o

üìê TIPOS:

HARD MARGIN:
‚Ä¢ Dados perfeitamente separ√°veis
‚Ä¢ Sem viola√ß√µes permitidas
‚Ä¢ Raramente poss√≠vel em dados reais

SOFT MARGIN:
‚Ä¢ Permite algumas viola√ß√µes
‚Ä¢ Par√¢metro C controla trade-off
‚Ä¢ C alto: menos viola√ß√µes, risco de overfit
‚Ä¢ C baixo: mais viola√ß√µes, mais generaliza√ß√£o

‚öôÔ∏è HIPERPAR√ÇMETROS:
‚Ä¢ C: penalidade por viola√ß√µes
‚Ä¢ kernel: tipo de kernel
‚Ä¢ gamma: par√¢metro para kernels RBF`,
            keyPoints: [
                "SVM maximiza margem entre classes",
                "Vetores de suporte definem a fronteira",
                "C alto: fit r√≠gido, C baixo: mais tolerante",
                "Funciona bem em alta dimensionalidade",
                "ESCALONAMENTO √â OBRIGAT√ìRIO"
            ],
            example: `from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

# OBRIGAT√ìRIO: escalonar dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVM Linear
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train_scaled, y_train)
print(f"Linear SVM: {svm_linear.score(X_test_scaled, y_test):.3f}")
print(f"Vetores de suporte: {svm_linear.n_support_}")

# Grid Search para hiperpar√¢metros
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto', 0.1, 1]
}

svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)
print(f"Melhores par√¢metros: {grid_search.best_params_}")
print(f"Melhor score: {grid_search.best_score_:.3f}")

# Visualizar fronteira (2D)
from mlxtend.plotting import plot_decision_regions
plot_decision_regions(X_train_scaled[:, :2], y_train, clf=svm_linear)`,
            realCase: {
                title: "Classifica√ß√£o de Texto e Spam",
                description: "SVMs foram estado da arte para classifica√ß√£o de texto antes de deep learning. Funcionam bem em alta dimensionalidade (milhares de features TF-IDF) onde outros algoritmos falham.",
                impact: "SpamAssassin e filtros de email usaram SVM por d√©cadas"
            }
        },
        {
            title: "Kernel Trick",
            concept: `Kernels permitem SVM capturar padr√µes n√£o-lineares:

üìä O PROBLEMA:
‚Ä¢ Dados frequentemente n√£o s√£o linearmente separ√°veis
‚Ä¢ Projetar para dimens√£o maior pode tornar separ√°veis
‚Ä¢ Mas computa√ß√£o expl√≠cita √© cara

üí° O TRUQUE:
‚Ä¢ Kernels computam produto interno no espa√ßo maior
‚Ä¢ SEM calcular coordenadas explicitamente
‚Ä¢ K(x, y) = œÜ(x) ¬∑ œÜ(y)

üìê KERNELS COMUNS:

LINEAR:
‚Ä¢ K(x, y) = x·µÄy
‚Ä¢ Para dados linearmente separ√°veis

RBF (Radial Basis Function):
‚Ä¢ K(x, y) = exp(-Œ≥||x-y||¬≤)
‚Ä¢ Projeta para dimens√£o INFINITA
‚Ä¢ Mais usado, funciona para maioria dos casos

POLYNOMIAL:
‚Ä¢ K(x, y) = (Œ≥x·µÄy + r)^d
‚Ä¢ Captura intera√ß√µes polinomiais
‚Ä¢ d = grau do polin√¥mio

SIGMOID:
‚Ä¢ K(x, y) = tanh(Œ≥x·µÄy + r)
‚Ä¢ Similar a rede neural

‚öôÔ∏è GAMMA (para RBF):
‚Ä¢ Alto: considera apenas vizinhos muito pr√≥ximos
‚Ä¢ Baixo: considera vizinhos distantes
‚Ä¢ Controla "alcance" do kernel`,
            keyPoints: [
                "Kernel trick evita computa√ß√£o expl√≠cita em alta dimens√£o",
                "RBF √© kernel padr√£o, funciona para maioria dos casos",
                "gamma controla alcance de influ√™ncia",
                "Polynomial captura intera√ß√µes de features",
                "Escolha de kernel via cross-validation"
            ],
            example: `from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt

# Comparar kernels
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
results = {}

for kernel in kernels:
    svm = SVC(kernel=kernel, C=1.0)
    svm.fit(X_train_scaled, y_train)
    acc = svm.score(X_test_scaled, y_test)
    results[kernel] = acc
    print(f"{kernel}: {acc:.3f}")

# Explorar gamma para RBF
gammas = [0.001, 0.01, 0.1, 1, 10]
for gamma in gammas:
    svm = SVC(kernel='rbf', gamma=gamma, C=1)
    svm.fit(X_train_scaled, y_train)
    train_acc = svm.score(X_train_scaled, y_train)
    test_acc = svm.score(X_test_scaled, y_test)
    print(f"gamma={gamma}: Train={train_acc:.3f}, Test={test_acc:.3f}")

# Visualizar efeito de gamma (dados 2D sint√©ticos)
from sklearn.datasets import make_circles
X_circle, y_circle = make_circles(n_samples=200, noise=0.1, factor=0.3)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for ax, gamma in zip(axes, [0.1, 1, 10]):
    svm = SVC(kernel='rbf', gamma=gamma)
    svm.fit(X_circle, y_circle)
    ax.scatter(X_circle[:, 0], X_circle[:, 1], c=y_circle)
    ax.set_title(f'gamma = {gamma}')`,
            realCase: {
                title: "Reconhecimento de D√≠gitos MNIST",
                description: "Antes de CNNs dominarem, SVMs com kernel RBF alcan√ßavam ~98% de precis√£o no MNIST. A capacidade de projetar para dimens√µes infinitas permitia separar d√≠gitos complexos.",
                impact: "SVM foi benchmark para classifica√ß√£o de imagens por anos"
            }
        },
        {
            title: "SVM for Regression (SVR)",
            concept: `SVM tamb√©m pode fazer regress√£o:

üìä DIFEREN√áA CONCEITUAL:
‚Ä¢ Classifica√ß√£o: maximiza margem entre classes
‚Ä¢ Regress√£o: cria "tubo" de toler√¢ncia Œµ

üìê EPSILON-INSENSITIVE:
‚Ä¢ Erros menores que Œµ s√£o ignorados
‚Ä¢ Apenas erros maiores que Œµ s√£o penalizados
‚Ä¢ Cria "tubo" ao redor da fun√ß√£o

‚öôÔ∏è PAR√ÇMETROS:

EPSILON (Œµ):
‚Ä¢ Largura do tubo de toler√¢ncia
‚Ä¢ Maior Œµ = mais toler√¢ncia a erros pequenos
‚Ä¢ Controla sparsidade dos vetores de suporte

C:
‚Ä¢ Penalidade por erros fora do tubo
‚Ä¢ Maior C = menos toler√¢ncia
‚Ä¢ Trade-off entre fit e generaliza√ß√£o

KERNEL:
‚Ä¢ Mesmos kernels de classifica√ß√£o
‚Ä¢ RBF mais comum para n√£o-linear

üí° QUANDO USAR:
‚Ä¢ Dados com outliers (robusto)
‚Ä¢ Problemas n√£o-lineares
‚Ä¢ Quando sparsidade √© desejada`,
            keyPoints: [
                "SVR cria tubo de toler√¢ncia ao redor da fun√ß√£o",
                "Erros dentro de Œµ s√£o ignorados",
                "C e Œµ controlam trade-off bias-variance",
                "Robusto a outliers (devido ao Œµ-tube)",
                "Kernels funcionam igual √† classifica√ß√£o"
            ],
            example: `from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Escalonar (OBRIGAT√ìRIO para SVR tamb√©m)
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_s = scaler_X.fit_transform(X_train)
y_train_s = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
X_test_s = scaler_X.transform(X_test)

# SVR b√°sico
svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)
svr.fit(X_train_s, y_train_s)

# Previs√µes (desfazer escala)
y_pred_s = svr.predict(X_test_s)
y_pred = scaler_y.inverse_transform(y_pred_s.reshape(-1, 1)).ravel()

print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"R¬≤: {r2_score(y_test, y_pred):.3f}")

# Grid Search para SVR
param_grid = {
    'C': [0.1, 1, 10],
    'epsilon': [0.01, 0.1, 0.5],
    'gamma': ['scale', 0.1, 1]
}

svr = SVR(kernel='rbf')
grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_s, y_train_s)

print(f"Melhores par√¢metros: {grid_search.best_params_}")
print(f"Melhor RMSE: {np.sqrt(-grid_search.best_score_):.3f}")

# Comparar com Linear Regression
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train, y_train)
print(f"Linear Regression R¬≤: {lr.score(X_test, y_test):.3f}")`,
            realCase: {
                title: "Previs√£o de Demanda de Energia",
                description: "Utilities usam SVR para prever demanda de energia, onde robustez a outliers (picos anormais) √© crucial. O Œµ-tube ignora varia√ß√µes normais, focando em tend√™ncias.",
                impact: "Previs√µes robustas para planejamento de capacidade energ√©tica"
            }
        }
    ]
};
