// CAIP-210 Study Content - Lessons 4-6
// Regression, Forecasting, Classification Models

// Lesson 4: Building Linear Regression Models
STUDY_CONTENT[4] = {
    name: "Lesson 4: Building Linear Regression Models",
    icon: "üìà",
    weight: "Focus Areas: Linear Algebra, Regularization, Gradient Descent",
    topics: [
        {
            title: "Linear Regression Fundamentals",
            concept: `Regress√£o Linear modela a rela√ß√£o entre vari√°veis independentes (features) e uma vari√°vel dependente cont√≠nua (target).

üìê EQUA√á√ÉO B√ÅSICA:
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô

Onde:
‚Ä¢ ≈∑ = valor previsto
‚Ä¢ Œ≤‚ÇÄ = intercepto (bias)
‚Ä¢ Œ≤‚ÇÅ...Œ≤‚Çô = coeficientes (pesos)
‚Ä¢ x‚ÇÅ...x‚Çô = features

üìä PRESSUPOSTOS:
1. Rela√ß√£o linear entre X e Y
2. Independ√™ncia dos erros
3. Homocedasticidade (vari√¢ncia constante)
4. Normalidade dos res√≠duos
5. Aus√™ncia de multicolinearidade

üéØ OBJETIVO:
Minimizar a Soma dos Quadrados dos Res√≠duos (RSS):
RSS = Œ£(y·µ¢ - ≈∑·µ¢)¬≤

EQUA√á√ÉO NORMAL (solu√ß√£o fechada):
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy`,
            keyPoints: [
                "Regress√£o linear assume rela√ß√£o linear entre features e target",
                "Coeficientes indicam quanto Y muda para cada unidade de X",
                "Equa√ß√£o normal funciona bem para datasets pequenos",
                "Multicolinearidade pode distorcer coeficientes",
                "R¬≤ mede propor√ß√£o da vari√¢ncia explicada"
            ],
            example: `import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Dados de exemplo: prever pre√ßo de casa
X = np.array([[1500, 3], [1800, 4], [2400, 4], [3000, 5], [3500, 5]])
y = np.array([300000, 350000, 450000, 550000, 600000])

# Treinar modelo
model = LinearRegression()
model.fit(X, y)

# Coeficientes
print(f"Intercepto: {model.intercept_:.0f}")
print(f"Coef. √°rea: {model.coef_[0]:.0f}/sqft")
print(f"Coef. quartos: {model.coef_[1]:.0f}/quarto")

# Previs√£o
nova_casa = [[2000, 4]]
preco_previsto = model.predict(nova_casa)
print(f"Pre√ßo previsto: {preco_previsto[0]:.0f}")

# M√©tricas
y_pred = model.predict(X)
print(f"R¬≤: {r2_score(y, y_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred)):.0f}")`,
            realCase: {
                title: "Zillow Zestimate",
                description: "O Zillow usa modelos de regress√£o com centenas de features (√°rea, localiza√ß√£o, caracter√≠sticas, vendas pr√≥ximas) para estimar valores de im√≥veis.",
                impact: "Estima valores de 100+ milh√µes de propriedades nos EUA com erro m√©dio de ~2%"
            }
        },
        {
            title: "Regulariza√ß√£o: Ridge e Lasso",
            concept: `Regulariza√ß√£o previne overfitting adicionando penalidade aos coeficientes:

üî∑ RIDGE REGRESSION (L2):
‚Ä¢ Adiciona penalidade ŒªŒ£Œ≤‚±º¬≤ √† fun√ß√£o de custo
‚Ä¢ Reduz coeficientes mas NUNCA zera
‚Ä¢ Bom quando todas as features s√£o relevantes
‚Ä¢ Par√¢metro Œ± controla for√ßa da regulariza√ß√£o

üî∂ LASSO REGRESSION (L1):
‚Ä¢ Adiciona penalidade ŒªŒ£|Œ≤‚±º| √† fun√ß√£o de custo
‚Ä¢ PODE zerar coeficientes (sele√ß√£o de features autom√°tica)
‚Ä¢ Bom quando muitas features s√£o irrelevantes
‚Ä¢ Produz modelos mais interpret√°veis

üî∑üî∂ ELASTIC NET:
‚Ä¢ Combina L1 e L2: Œª‚ÇÅŒ£|Œ≤‚±º| + Œª‚ÇÇŒ£Œ≤‚±º¬≤
‚Ä¢ Par√¢metro l1_ratio controla propor√ß√£o
‚Ä¢ √ötil quando features s√£o correlacionadas

üìä ESCOLHENDO Œ± (regulariza√ß√£o):
‚Ä¢ Œ± muito baixo ‚Üí overfitting
‚Ä¢ Œ± muito alto ‚Üí underfitting
‚Ä¢ Use cross-validation para encontrar Œ± √≥timo`,
            keyPoints: [
                "Regulariza√ß√£o adiciona vi√©s para reduzir vari√¢ncia",
                "Ridge: todos os coeficientes encolhem mas n√£o zeram",
                "Lasso: pode zerar coeficientes ‚Üí sele√ß√£o de features",
                "Elastic Net: combina√ß√£o de L1 e L2",
                "Cross-validation essencial para escolher Œ±"
            ],
            example: `from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import cross_val_score
import numpy as np

# Comparar regulariza√ß√µes
models = {
    'Linear': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)
}

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    print(f"{name}: R¬≤ = {scores.mean():.3f} (+/- {scores.std()*2:.3f})")

# Encontrar melhor alpha via CV
from sklearn.linear_model import RidgeCV, LassoCV

ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100])
ridge_cv.fit(X, y)
print(f"Melhor alpha Ridge: {ridge_cv.alpha_}")

lasso_cv = LassoCV(alphas=[0.001, 0.01, 0.1, 1], cv=5)
lasso_cv.fit(X, y)
print(f"Melhor alpha Lasso: {lasso_cv.alpha_}")

# Lasso zera coeficientes irrelevantes
print(f"Coeficientes Lasso: {lasso_cv.coef_}")`,
            realCase: {
                title: "Sele√ß√£o de Genes em Bioinform√°tica",
                description: "Pesquisadores usam Lasso para identificar quais genes (de milhares) s√£o relevantes para prever doen√ßas.",
                impact: "Reduziu an√°lises de 20.000 genes para dezenas de genes relevantes"
            }
        },
        {
            title: "Gradient Descent",
            concept: `Gradient Descent √© um m√©todo iterativo para minimizar fun√ß√µes de custo:

üîÑ ALGORITMO:
1. Inicializar pesos aleatoriamente
2. Calcular gradiente da fun√ß√£o de custo
3. Atualizar pesos: w = w - Œ± √ó ‚àáJ(w)
4. Repetir at√© converg√™ncia

üìä VARIANTES:

BATCH GRADIENT DESCENT (BGD):
‚Ä¢ Usa TODO o dataset para calcular gradiente
‚Ä¢ Converg√™ncia est√°vel mas lenta
‚Ä¢ Mem√≥ria: precisa de todo dataset na RAM

STOCHASTIC GRADIENT DESCENT (SGD):
‚Ä¢ Usa UM exemplo por itera√ß√£o
‚Ä¢ Converg√™ncia r√°pida mas ruidosa
‚Ä¢ Pode escapar de m√≠nimos locais

MINI-BATCH GRADIENT DESCENT:
‚Ä¢ Usa LOTE de exemplos (32, 64, 128...)
‚Ä¢ Equil√≠brio entre velocidade e estabilidade
‚Ä¢ Mais usado na pr√°tica

‚öôÔ∏è HIPERPAR√ÇMETROS:
‚Ä¢ Learning rate (Œ±): tamanho do passo
‚Ä¢ Epochs: passagens pelo dataset
‚Ä¢ Batch size: exemplos por itera√ß√£o`,
            keyPoints: [
                "Gradient descent encontra m√≠nimos iterativamente",
                "Learning rate muito alto ‚Üí oscila√ß√£o, muito baixo ‚Üí lentid√£o",
                "Batch GD: est√°vel mas lento para grandes datasets",
                "SGD: r√°pido mas ruidoso, pode escapar m√≠nimos locais",
                "Mini-batch: padr√£o para deep learning"
            ],
            example: `import numpy as np

def gradient_descent(X, y, learning_rate=0.01, epochs=1000):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0
    
    for epoch in range(epochs):
        # Previs√£o: ≈∑ = Xw + b
        y_pred = np.dot(X, weights) + bias
        
        # Gradientes
        dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
        db = (1/n_samples) * np.sum(y_pred - y)
        
        # Atualizar pesos
        weights -= learning_rate * dw
        bias -= learning_rate * db
        
        if epoch % 100 == 0:
            mse = np.mean((y_pred - y)**2)
            print(f"Epoch {epoch}: MSE = {mse:.4f}")
    
    return weights, bias

# Usar SGDRegressor do sklearn
from sklearn.linear_model import SGDRegressor

sgd = SGDRegressor(
    loss='squared_error',
    learning_rate='adaptive',
    eta0=0.01,
    max_iter=1000,
    early_stopping=True
)
sgd.fit(X_scaled, y)`,
            realCase: {
                title: "Treinamento de GPT",
                description: "Modelos de linguagem como GPT usam variantes de SGD (como Adam) para ajustar bilh√µes de par√¢metros.",
                impact: "Adam optimizer √© o padr√£o para treinar redes neurais modernas"
            }
        }
    ]
};

// Lesson 5: Building Forecasting Models
STUDY_CONTENT[5] = {
    name: "Lesson 5: Building Forecasting Models",
    icon: "üìÖ",
    weight: "Focus Areas: Time Series, ARIMA, Multivariate Forecasting",
    topics: [
        {
            title: "Time Series Fundamentals",
            concept: `S√©ries temporais s√£o sequ√™ncias de dados ordenados no tempo:

üìä COMPONENTES:
‚Ä¢ TEND√äNCIA: Dire√ß√£o geral ao longo do tempo (alta/baixa)
‚Ä¢ SAZONALIDADE: Padr√µes que repetem em intervalos fixos
‚Ä¢ CICLO: Flutua√ß√µes de longo prazo (n√£o fixas)
‚Ä¢ RU√çDO: Varia√ß√£o aleat√≥ria residual

üîç AN√ÅLISE EXPLORAT√ìRIA:
1. Plotar s√©rie ao longo do tempo
2. Identificar tend√™ncia visual
3. Detectar padr√µes sazonais
4. Verificar outliers e mudan√ßas

üìà ESTACIONARIEDADE:
Uma s√©rie √© estacion√°ria quando:
‚Ä¢ M√©dia constante ao longo do tempo
‚Ä¢ Vari√¢ncia constante
‚Ä¢ Autocovari√¢ncia n√£o depende do tempo

Testes: ADF (Augmented Dickey-Fuller), KPSS

üîß TRANSFORMA√á√ïES:
‚Ä¢ Diferencia√ß√£o: y‚Çú' = y‚Çú - y‚Çú‚Çã‚ÇÅ
‚Ä¢ Log: estabiliza vari√¢ncia
‚Ä¢ Decomposi√ß√£o: separa componentes`,
            keyPoints: [
                "S√©ries temporais t√™m ordem temporal significativa",
                "Componentes: tend√™ncia, sazonalidade, ciclo, ru√≠do",
                "Estacionariedade √© requisito para muitos modelos",
                "Diferencia√ß√£o remove tend√™ncia",
                "Decomposi√ß√£o ajuda a entender estrutura"
            ],
            example: `import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

# Carregar dados de vendas mensais
dates = pd.date_range('2020-01-01', periods=36, freq='M')
sales = [100+i*2+10*np.sin(i/2)+np.random.randn()*5 for i in range(36)]
ts = pd.Series(sales, index=dates)

# Decomposi√ß√£o
decomposition = seasonal_decompose(ts, model='additive', period=12)

# Teste de estacionariedade (ADF)
result = adfuller(ts)
print(f'ADF Statistic: {result[0]:.4f}')
print(f'p-value: {result[1]:.4f}')
if result[1] < 0.05:
    print("S√©rie √© ESTACION√ÅRIA")
else:
    print("S√©rie N√ÉO ESTACION√ÅRIA - aplicar diferencia√ß√£o")
    
# Diferencia√ß√£o
ts_diff = ts.diff().dropna()`,
            realCase: {
                title: "Previs√£o de Demanda na Amazon",
                description: "Amazon usa modelos de s√©ries temporais para prever demanda por milh√µes de produtos. Sazonalidade (Black Friday, Natal) e tend√™ncias s√£o cr√≠ticas.",
                impact: "Previs√µes precisas economizam bilh√µes em custos de estoque e envio"
            }
        },
        {
            title: "ARIMA Models",
            concept: `ARIMA combina tr√™s componentes para previs√£o:

üìä ARIMA(p, d, q):

AR (AutoRegressive) - p:
‚Ä¢ Usa valores passados para prever
‚Ä¢ y‚Çú = c + œÜ‚ÇÅy‚Çú‚Çã‚ÇÅ + œÜ‚ÇÇy‚Çú‚Çã‚ÇÇ + ... + Œµ‚Çú
‚Ä¢ p = n√∫mero de lags

I (Integrated) - d:
‚Ä¢ Ordem de diferencia√ß√£o
‚Ä¢ d=1: uma diferencia√ß√£o
‚Ä¢ Torna s√©rie estacion√°ria

MA (Moving Average) - q:
‚Ä¢ Usa erros passados
‚Ä¢ y‚Çú = c + Œµ‚Çú + Œ∏‚ÇÅŒµ‚Çú‚Çã‚ÇÅ + Œ∏‚ÇÇŒµ‚Çú‚Çã‚ÇÇ
‚Ä¢ q = n√∫mero de lags de erro

üìà SARIMA para sazonalidade:
SARIMA(p,d,q)(P,D,Q,s)
‚Ä¢ s = per√≠odo sazonal (12 para mensal)
‚Ä¢ P, D, Q = componentes sazonais

üîç ESCOLHENDO PAR√ÇMETROS:
‚Ä¢ ACF plot ‚Üí determina q
‚Ä¢ PACF plot ‚Üí determina p
‚Ä¢ Auto ARIMA: busca autom√°tica`,
            keyPoints: [
                "AR: previs√£o baseada em valores passados",
                "I: diferencia√ß√£o para estacionariedade",
                "MA: previs√£o baseada em erros passados",
                "SARIMA adiciona componentes sazonais",
                "ACF e PACF ajudam a identificar p e q"
            ],
            example: `from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pmdarima as pm

# ARIMA manual
model = ARIMA(ts, order=(1, 1, 1))
fitted = model.fit()
print(fitted.summary())

# Previs√£o
forecast = fitted.forecast(steps=6)
print("Pr√≥ximos 6 meses:", forecast.values)

# Auto ARIMA - encontra melhor modelo automaticamente
auto_model = pm.auto_arima(
    ts,
    seasonal=True, m=12,
    stepwise=True,
    suppress_warnings=True,
    trace=True
)
print(f"Melhor modelo: {auto_model.order} x {auto_model.seasonal_order}")

# SARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

sarima = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12))
sarima_fit = sarima.fit(disp=False)
sarima_forecast = sarima_fit.forecast(steps=12)`,
            realCase: {
                title: "Previs√£o de Vendas no Walmart",
                description: "Walmart usa modelos SARIMA e variantes para prever vendas semanais por loja.",
                impact: "Top solutions combinaram ARIMA com gradient boosting para capturar padr√µes sazonais"
            }
        },
        {
            title: "Multivariate Time Series",
            concept: `Quando m√∫ltiplas s√©ries temporais se influenciam mutuamente:

üìä VAR (Vector AutoRegression):
‚Ä¢ Estende AR para m√∫ltiplas vari√°veis
‚Ä¢ Cada vari√°vel depende de seus lags E lags das outras
‚Ä¢ Captura interdepend√™ncias

Exemplo: PIB e Taxa de Juros se influenciam mutuamente

üìà EXOGENOUS VARIABLES (SARIMAX):
‚Ä¢ Vari√°veis externas que afetam a s√©rie
‚Ä¢ N√£o s√£o previstas, s√£o fornecidas
‚Ä¢ Ex: temperatura ‚Üí vendas de sorvete

üîç COINTEGRA√á√ÉO:
‚Ä¢ S√©ries n√£o estacion√°rias que se movem juntas
‚Ä¢ Rela√ß√£o de longo prazo est√°vel
‚Ä¢ Importante para economia/finan√ßas

‚öôÔ∏è ABORDAGEM PR√ÅTICA:
1. Testar estacionariedade de cada s√©rie
2. Verificar causalidade de Granger
3. Selecionar ordem do VAR (AIC/BIC)
4. Validar com forecasting out-of-sample`,
            keyPoints: [
                "VAR modela m√∫ltiplas s√©ries interdependentes",
                "Cada s√©rie depende de lags pr√≥prios e das outras",
                "SARIMAX adiciona vari√°veis ex√≥genas ao ARIMA",
                "Granger causality testa se uma s√©rie prev√™ outra",
                "Cointegra√ß√£o indica rela√ß√£o de longo prazo"
            ],
            example: `from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests

# Dados multivariados
data = pd.DataFrame({
    'sales': sales_series,
    'marketing': marketing_series,
    'temperature': temp_series
})

# Teste de causalidade de Granger
granger_test = grangercausalitytests(
    data[['sales', 'marketing']], 
    maxlag=4, 
    verbose=True
)

# Modelo VAR
model = VAR(data[['sales', 'marketing']])

# Selecionar ordem √≥tima
for i in range(1, 11):
    result = model.fit(i)
    print(f'Lag {i}: AIC={result.aic:.2f}, BIC={result.bic:.2f}')

# Ajustar VAR
var_result = model.fit(maxlags=4, ic='aic')

# SARIMAX com vari√°vel ex√≥gena
sarimax = SARIMAX(
    sales_series,
    exog=marketing_series,
    order=(1,1,1),
    seasonal_order=(1,1,1,12)
)
sarimax_fit = sarimax.fit()`,
            realCase: {
                title: "Previs√£o Econ√¥mica do Federal Reserve",
                description: "O Federal Reserve usa modelos VAR para prever PIB, infla√ß√£o e desemprego simultaneamente.",
                impact: "Modelos VAR s√£o fundamentais para pol√≠tica macroecon√¥mica global"
            }
        }
    ]
};

// Lesson 6: Classification Models
STUDY_CONTENT[6] = {
    name: "Lesson 6: Classification with Logistic Regression & k-NN",
    icon: "üéØ",
    weight: "Focus Areas: Logistic Regression, k-NN, Multi-class, Evaluation",
    topics: [
        {
            title: "Logistic Regression",
            concept: `Apesar do nome, Logistic Regression √© para CLASSIFICA√á√ÉO:

üìä FUN√á√ÉO SIGMOID:
œÉ(z) = 1 / (1 + e‚Åª·∂ª)
‚Ä¢ Converte qualquer valor para [0, 1]
‚Ä¢ Interpretado como probabilidade

üìê MODELO:
P(y=1|x) = œÉ(Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ...)

üéØ DECIS√ÉO:
‚Ä¢ Se P(y=1) > threshold ‚Üí classe 1
‚Ä¢ Threshold padr√£o = 0.5
‚Ä¢ Pode ajustar para precision/recall

üìà TREINAMENTO:
‚Ä¢ Usa Maximum Likelihood Estimation
‚Ä¢ Otimiza Log-Loss (Cross-Entropy)
‚Ä¢ Log-Loss = -Œ£[y¬∑log(p) + (1-y)¬∑log(1-p)]

üí° INTERPRETA√á√ÉO:
‚Ä¢ Coeficientes = log odds ratio
‚Ä¢ exp(Œ≤) = quanto odds multiplicam para +1 unidade
‚Ä¢ Altamente interpret√°vel!

‚ö†Ô∏è LIMITA√á√ïES:
‚Ä¢ Assume rela√ß√£o linear no log-odds
‚Ä¢ N√£o captura intera√ß√µes automaticamente
‚Ä¢ Sens√≠vel a outliers`,
            keyPoints: [
                "Sigmoid transforma output em probabilidade [0,1]",
                "Otimiza log-loss, n√£o MSE",
                "Coeficientes interpret√°veis como log odds",
                "Threshold ajust√°vel baseado no problema",
                "Base para redes neurais (neur√¥nio sigmoid)"
            ],
            example: `from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
import numpy as np

# Dados de exemplo: prever se cliente compra
X = np.array([[25, 30000], [35, 50000], [45, 80000], 
              [20, 20000], [50, 100000], [30, 40000]])
y = np.array([0, 0, 1, 0, 1, 0])

# Treinar
model = LogisticRegression()
model.fit(X, y)

# Probabilidades
probs = model.predict_proba(X)
print("Probabilidades de compra:", probs[:, 1])

# Coeficientes (interpreta√ß√£o)
print(f"Coef. idade: {model.coef_[0][0]:.4f}")
print(f"Coef. renda: {model.coef_[0][1]:.8f}")
print(f"Odds ratio renda: {np.exp(model.coef_[0][1]*10000):.2f}x por +10k")

# Ajustar threshold para mais recall
threshold = 0.3
y_pred_custom = (probs[:, 1] >= threshold).astype(int)

# M√©tricas
from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y, probs[:, 1])`,
            realCase: {
                title: "Credit Scoring em Bancos",
                description: "Bancos usam logistic regression para credit scoring por sua interpretabilidade. Reguladores exigem decis√µes de cr√©dito explic√°veis.",
                impact: "Decis√µes de cr√©dito transparentes e audit√°veis, exigidas por regulamenta√ß√£o"
            }
        },
        {
            title: "k-Nearest Neighbors (k-NN)",
            concept: `k-NN classifica baseado nos vizinhos mais pr√≥ximos:

üìä ALGORITMO:
1. Calcular dist√¢ncia at√© todos os pontos de treino
2. Selecionar k vizinhos mais pr√≥ximos
3. Votar pela classe mais frequente

üìê M√âTRICAS DE DIST√ÇNCIA:
‚Ä¢ Euclidiana: ‚àöŒ£(x·µ¢ - y·µ¢)¬≤
‚Ä¢ Manhattan: Œ£|x·µ¢ - y·µ¢|
‚Ä¢ Minkowski: generaliza√ß√£o

‚öôÔ∏è ESCOLHENDO k:
‚Ä¢ k pequeno: mais sens√≠vel a ru√≠do
‚Ä¢ k grande: mais suave mas perde detalhes
‚Ä¢ Usar CV para encontrar k √≥timo
‚Ä¢ k √≠mpar evita empates

‚ö†Ô∏è CARACTER√çSTICAS:
‚Ä¢ N√£o-param√©trico (sem modelo fixo)
‚Ä¢ "Lazy learner": n√£o treina, s√≥ memoriza
‚Ä¢ Sens√≠vel √† escala ‚Üí NORMALIZAR!
‚Ä¢ Lento para grandes datasets
‚Ä¢ Maldi√ß√£o da dimensionalidade

üí° QUANDO USAR:
‚Ä¢ Datasets pequenos/m√©dios
‚Ä¢ Fronteiras de decis√£o n√£o lineares
‚Ä¢ Como baseline simples`,
            keyPoints: [
                "Classifica pelo voto dos k vizinhos mais pr√≥ximos",
                "ESCALONAMENTO √â OBRIGAT√ìRIO",
                "k pequeno = overfit, k grande = underfit",
                "Lazy learner: lento na previs√£o",
                "Sofre com maldi√ß√£o da dimensionalidade"
            ],
            example: `from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

# IMPORTANTE: Escalonar features!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encontrar melhor k
k_range = range(1, 21)
cv_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# Melhor k
best_k = k_range[np.argmax(cv_scores)]
print(f"Melhor k: {best_k}")

# Modelo final
knn = KNeighborsClassifier(
    n_neighbors=best_k,
    weights='distance',
    metric='euclidean'
)
knn.fit(X_scaled, y)

# Previs√£o
novo_cliente = scaler.transform([[40, 60000]])
print(f"Classe: {knn.predict(novo_cliente)}")`,
            realCase: {
                title: "Sistemas de Recomenda√ß√£o Colaborativa",
                description: "Netflix usou variantes de k-NN em seu sistema de recomenda√ß√£o original. O Netflix Prize foi vencido usando ensemble incluindo k-NN.",
                impact: "k-NN colaborativo foi funda√ß√£o para sistemas de recomenda√ß√£o modernos"
            }
        },
        {
            title: "Multi-class Classification",
            concept: `Quando h√° 3 ou mais classes para prever:

üìä ESTRAT√âGIAS:

ONE-VS-REST (OvR):
‚Ä¢ Treina N classificadores bin√°rios
‚Ä¢ Cada um: "classe i vs todas as outras"
‚Ä¢ Prev√™ classe com maior confian√ßa
‚Ä¢ Mais comum, eficiente

ONE-VS-ONE (OvO):
‚Ä¢ Treina N(N-1)/2 classificadores
‚Ä¢ Cada par de classes
‚Ä¢ Vota√ß√£o para classe final
‚Ä¢ Melhor para SVMs

MULTINOMIAL (Softmax):
‚Ä¢ Um modelo com N outputs
‚Ä¢ Softmax: e·∂ª‚Å± / Œ£e·∂ª ≤
‚Ä¢ Probabilidades somam 1
‚Ä¢ Usado em redes neurais

üìà M√âTRICAS MULTI-CLASS:

Macro Average:
‚Ä¢ M√©dia simples por classe
‚Ä¢ Trata classes igualmente

Weighted Average:
‚Ä¢ M√©dia ponderada por suporte
‚Ä¢ Considera desbalanceamento

Micro Average:
‚Ä¢ Agregado global
‚Ä¢ Igual a accuracy global`,
            keyPoints: [
                "OvR: N classificadores, um por classe",
                "OvO: N(N-1)/2 classificadores, cada par",
                "Softmax: probabilidades multi-classe",
                "Macro: m√©dia igual por classe",
                "Weighted: considera propor√ß√£o de classes"
            ],
            example: `from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier

# Dados multi-classe (3 classes)
X_multi = np.random.randn(150, 4)
y_multi = np.repeat([0, 1, 2], 50)

# Op√ß√£o 1: Multinomial nativo
lr_multi = LogisticRegression(multi_class='multinomial', max_iter=1000)
lr_multi.fit(X_multi, y_multi)
print("Probabilidades softmax:", lr_multi.predict_proba(X_multi[:1]))

# Op√ß√£o 2: One-vs-Rest expl√≠cito
ovr = OneVsRestClassifier(LogisticRegression())
ovr.fit(X_multi, y_multi)

# Op√ß√£o 3: One-vs-One expl√≠cito
ovo = OneVsOneClassifier(LogisticRegression())
ovo.fit(X_multi, y_multi)

# M√©tricas multi-classe
y_pred = lr_multi.predict(X_multi)
print(classification_report(y_multi, y_pred, 
                            target_names=['Classe 0', 'Classe 1', 'Classe 2']))

# Confusion matrix multi-classe
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_multi, y_pred)
ConfusionMatrixDisplay(cm, display_labels=['C0', 'C1', 'C2']).plot()`,
            realCase: {
                title: "Classifica√ß√£o de Imagens ImageNet",
                description: "ImageNet tem 1000 classes de objetos. Modelos como ResNet usam softmax final com 1000 outputs.",
                impact: "Softmax multi-classe √© padr√£o para classifica√ß√£o de imagens"
            }
        },
        {
            title: "Classification Metrics Deep Dive",
            concept: `M√©tricas detalhadas para avaliar classificadores:

üìä CONFUSION MATRIX:
                Predicted
                Neg    Pos
Actual  Neg     TN     FP    ‚Üê Specificity = TN/(TN+FP)
        Pos     FN     TP    ‚Üê Recall = TP/(TP+FN)
                ‚Üì      ‚Üì
             NPV   Precision

üìà CURVAS:

ROC CURVE:
‚Ä¢ Eixo X: False Positive Rate (1 - Specificity)
‚Ä¢ Eixo Y: True Positive Rate (Recall)
‚Ä¢ AUC: √°rea sob a curva
‚Ä¢ AUC = 0.5: random, AUC = 1: perfeito

PR CURVE (Precision-Recall):
‚Ä¢ Eixo X: Recall
‚Ä¢ Eixo Y: Precision
‚Ä¢ Melhor para dados desbalanceados
‚Ä¢ AP: Average Precision

üéØ QUANDO USAR CADA:
‚Ä¢ Accuracy: dados balanceados
‚Ä¢ Precision: custo alto de FP (spam filter)
‚Ä¢ Recall: custo alto de FN (diagn√≥stico m√©dico)
‚Ä¢ F1: equil√≠brio precision-recall
‚Ä¢ AUC-ROC: comparar modelos geralmente
‚Ä¢ AUC-PR: dados muito desbalanceados`,
            keyPoints: [
                "ROC-AUC bom para comparar modelos geralmente",
                "PR-AUC melhor para dados desbalanceados",
                "Threshold afeta precision-recall tradeoff",
                "F1 √© m√©dia harm√¥nica de precision e recall",
                "Escolha de m√©trica depende do custo de erros"
            ],
            example: `from sklearn.metrics import (roc_curve, auc, precision_recall_curve,
                               average_precision_score, f1_score)
import matplotlib.pyplot as plt

# Obter probabilidades
y_proba = model.predict_proba(X_test)[:, 1]

# ROC Curve
fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

# PR Curve
precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)

plt.subplot(1, 2, 2)
plt.plot(recall, precision, label=f'PR (AP = {ap:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()

# Encontrar threshold √≥timo para F1
f1_scores = []
for thresh in thresholds_pr:
    y_pred_temp = (y_proba >= thresh).astype(int)
    f1_scores.append(f1_score(y_test, y_pred_temp))
    
best_thresh = thresholds_pr[np.argmax(f1_scores)]
print(f"Threshold √≥timo para F1: {best_thresh:.3f}")`,
            realCase: {
                title: "Detec√ß√£o de Fraude em Cart√µes",
                description: "Com apenas 0.1% de transa√ß√µes fraudulentas, accuracy √© in√∫til. Bancos otimizam para recall alto enquanto mant√™m precision aceit√°vel.",
                impact: "AUC-PR e recall s√£o m√©tricas principais, n√£o accuracy"
            }
        }
    ]
};
